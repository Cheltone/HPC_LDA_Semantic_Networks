{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ae4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "import re\n",
    "#import en_core_web_sm\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import numpy as np #For divergences/distances\n",
    "import scipy as sp#For divergences/distances\n",
    "import sklearn.manifold #For a manifold plot\n",
    "import urllib.parse #For joining urls\n",
    "import matplotlib.colors\n",
    "import graphviz \n",
    "import sklearn\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.decomposition\n",
    "import collections\n",
    "import os.path\n",
    "import random\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import networkx as nx\n",
    "import logging\n",
    "import pickle #if you want to save layouts\n",
    "\n",
    "%matplotlib inline\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "#from textblob import TextBlob\n",
    "import sys\n",
    "#import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "plt.style.use('fivethirtyeight')\n",
    "import matplotlib.ticker as mticker\n",
    "from scipy import stats\n",
    "import sklearn.feature_extraction\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "import emoji\n",
    "\n",
    "#Courtesy of Computational-Content-Analysis-2020. This package is not up-to-date at the link and will take some alteration locally\n",
    "import lucem_illud_2020 #pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n",
    "\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "# libraries for visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from gensim import corpora\n",
    "from datetime import datetime\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae40952",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [#'Jan2020',\n",
    "#'Apr2020',\n",
    "#'Feb2021',\n",
    "#'Jun2020',\n",
    "#'Nov2020',\n",
    "#'May2020',\n",
    "#'Mar2021',\n",
    "#'Apr2021',\n",
    "#'Oct2020',\n",
    "#'Jul2021',\n",
    "#'Sep2020',\n",
    "#'Sep2021',\n",
    "#'Aug2020',\n",
    "#'May2021',\n",
    "#'Dec2020',\n",
    "#'Feb2020',\n",
    "#'Aug2021',\n",
    "#'Jan2021',\n",
    "#'Jan2022',\n",
    "#'Jun2021',\n",
    "#'Feb2022',\n",
    "#'Oct2021',\n",
    "#'Jul2020',\n",
    "#'Nov2021',\n",
    "#'Mar2020',\n",
    "'Dec2021',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5119a4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dec2021\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f)\n",
    "    \n",
    "    comment_DF = pd.read_csv(\"%s.csv\" %f)\n",
    "    len(comment_DF)\n",
    "    logger = logging.getLogger(\"spacy\")\n",
    "    logger.setLevel(logging.ERROR)\n",
    "    \n",
    "    def tokeandnorm(comment_DF):\n",
    "        comment_DF['tokenized_sents'] = comment_DF['comment'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "        comment_DF['normalized_sents'] = comment_DF['tokenized_sents'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s) for s in x])\n",
    "        return comment_DF\n",
    "\n",
    "\n",
    "    def parallelize_dataframe(comment_DF, func, n_cores=48):\n",
    "        df_split = np.array_split(comment_DF, n_cores)\n",
    "        pool = Pool(n_cores)\n",
    "        comment_DF = pd.concat(pool.map(func, df_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return comment_DF\n",
    "\n",
    "    comment_DF = parallelize_dataframe(comment_DF, tokeandnorm)    \n",
    "\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    \n",
    "    #Cleaning CLEAN MORE\n",
    "    def cleanTxt(text):\n",
    "        \n",
    "        text = re.sub(r'@[A-Za-z0-9]+', '', str(text)) #\n",
    "        text = re.sub(r'#', '', text)\n",
    "        text = re.sub(r'RT[\\s]+', '', str(text))\n",
    "        text = re.sub(r'https?:\\/\\/\\S+', '', str(text))\n",
    "        text = re.sub(r'>', '', str(text))\n",
    "        text = re.sub(r'<', '', str(text))\n",
    "        text = re.sub(r'=', '', str(text))\n",
    "        text = re.sub(r'|', '', str(text))\n",
    "        text = re.sub(r'jeffrey', '', str(text))\n",
    "        text = re.sub(r'epstein', '', str(text))\n",
    "        text = re.sub(r'et', '', str(text))\n",
    "        text = re.sub(r'al', '', str(text))\n",
    "        text =  re.sub(emoji.get_emoji_regexp(), r\"\", text)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        return text\n",
    "    \n",
    "    def cleanit(comment_DF):\n",
    "        comment_DF['normalized_sents'] = comment_DF['normalized_sents'].apply(cleanTxt)\n",
    "        return comment_DF\n",
    "\n",
    "    def parallelize_dataframe(comment_DF, func, n_cores=48):\n",
    "        df_split = np.array_split(comment_DF, n_cores)\n",
    "        pool = Pool(n_cores)\n",
    "        comment_DF = pd.concat(pool.map(func, df_split))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        return comment_DF\n",
    "    \n",
    "    comment_DF = parallelize_dataframe(comment_DF, cleanit)\n",
    "\n",
    "    \n",
    "    PosDF = comment_DF.loc[comment_DF['label'] >= 'LABEL_1']\n",
    "    NegDF = comment_DF.loc[comment_DF['label'] >= 'LABEL_0']\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    Pos_text_list = PosDF['normalized_sents'].tolist()\n",
    "    Neg_text_list = NegDF['normalized_sents'].tolist()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): \n",
    "        output = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(sent) \n",
    "            output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n",
    "        return output\n",
    "\n",
    "    Pos_tokenized_reviews = lemmatization(Pos_text_list)\n",
    "    Neg_tokenized_reviews = lemmatization(Neg_text_list)\n",
    "    \n",
    "    \n",
    "    Pos_dictionary = corpora.Dictionary(Pos_tokenized_reviews)\n",
    "    Neg_dictionary = corpora.Dictionary(Neg_tokenized_reviews)\n",
    "    \n",
    "    Pos_doc_term_matrix = [Pos_dictionary.doc2bow(word) for word in Pos_tokenized_reviews]\n",
    "    Neg_doc_term_matrix = [Neg_dictionary.doc2bow(word) for word in Neg_tokenized_reviews]\n",
    "    \n",
    "\n",
    "    Poslda = LdaMulticore(corpus=Pos_doc_term_matrix, id2word=Pos_dictionary, num_topics=8, workers = 48)\n",
    "    Neglda = LdaMulticore(corpus=Neg_doc_term_matrix, id2word=Neg_dictionary, num_topics=8, workers = 48)\n",
    "\n",
    "    Postopics = Poslda.print_topics(num_words=10)\n",
    "    Negtopics = Neglda.print_topics(num_words=10)\n",
    "    \n",
    "    PosdataList = list(Postopics)\n",
    "    NegdataList = list(Negtopics)\n",
    "\n",
    "    dfPos = pd.DataFrame(PosdataList, columns=['Topic', 'Words and probability'])\n",
    "    dfNeg = pd.DataFrame(NegdataList, columns=['Topic', 'Words and probability'])\n",
    "    \n",
    "\n",
    "    dfPos.to_csv('Pos_LDA_Twitter%s.csv' %f)\n",
    "    dfNeg.to_csv('Neg_LDA_Twitter%s.csv' %f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a115484",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SA_Shiz)",
   "language": "python",
   "name": "sa_shiz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
